{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§¬ **Yapay Zeka Destekli Ä°laÃ§ KeÅŸfi: ABL1, FLT3, BTK, KIT, BCL2, JAK2 Hedefli MolekÃ¼l Ãœretimi**\n",
        "\n",
        "Bu proje, **Kronik Miyeloid LÃ¶semi (CML)** tedavisinde kritik bir hedef olan **ABL1, FLT3, BTK, KIT, BCL2, JAK2** proteinini baskÄ±layabilecek (inhibitÃ¶r) yeni ilaÃ§ aday molekÃ¼llerini **Generative AI (Ãœretken Yapay Zeka)** kullanarak tasarlamayÄ± amaÃ§lamaktadÄ±r.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ **Proje AkÄ±ÅŸÄ±:**\n",
        "1.  **Biyoinformatik:** NCBI'dan hedef protein sekansÄ±nÄ±n Ã§ekilmesi.\n",
        "2.  **Veri MadenciliÄŸi:** ChEMBL veritabanÄ±ndan bu hedefe etki eden ilaÃ§larÄ±n bulunmasÄ±.\n",
        "3.  **Keminformatik:** RDKit ile molekÃ¼llerin temizlenmesi ve SMILES formatÄ±na Ã§evrilmesi.\n",
        "4.  **Deep Learning:** PyTorch ile LSTM tabanlÄ± bir dil modeli eÄŸitilmesi.\n",
        "5.  **De Novo TasarÄ±m:** EÄŸitilen model ile tamamen yeni molekÃ¼llerin Ã¼retilmesi.\n",
        "\n",
        "### ğŸ› ï¸ **KullanÄ±lan KÃ¼tÃ¼phaneler:**\n",
        "<span style=\"background-color: #f1c40f; color: black; padding: 4px 8px; border-radius: 4px; font-weight: bold; margin-right:5px;\">BioPython</span>\n",
        "<span style=\"background-color: #008080; color: white; padding: 4px 8px; border-radius: 4px; font-weight: bold; margin-right:5px;\">ChEMBL Web Resource</span>\n",
        "<span style=\"background-color: #3498db; color: white; padding: 4px 8px; border-radius: 4px; font-weight: bold; margin-right:5px;\">RDKit</span>\n",
        "<span style=\"background-color: #e74c3c; color: white; padding: 4px 8px; border-radius: 4px; font-weight: bold; margin-right:5px;\">PyTorch</span>"
      ],
      "metadata": {
        "id": "MgrCmzYc8uyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyNHvrjZ8pOx",
        "outputId": "101c8347-6cc8-4a52-df48-4085e0d715db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.86\n",
            "Collecting chembl-webresource-client\n",
            "  Downloading chembl_webresource_client-0.10.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from chembl-webresource-client) (2.5.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.12/dist-packages (from chembl-webresource-client) (2.32.4)\n",
            "Collecting requests-cache~=1.2 (from chembl-webresource-client)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from chembl-webresource-client) (1.13)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->chembl-webresource-client) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->chembl-webresource-client) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->chembl-webresource-client) (2025.11.12)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.12/dist-packages (from requests-cache~=1.2->chembl-webresource-client) (25.4.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache~=1.2->chembl-webresource-client)\n",
            "  Downloading cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-cache~=1.2->chembl-webresource-client) (4.5.1)\n",
            "Collecting url-normalize>=1.4 (from requests-cache~=1.2->chembl-webresource-client)\n",
            "  Downloading url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from cattrs>=22.2->requests-cache~=1.2->chembl-webresource-client) (4.15.0)\n",
            "Downloading chembl_webresource_client-0.10.9-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: url-normalize, cattrs, requests-cache, chembl-webresource-client\n",
            "Successfully installed cattrs-25.3.0 chembl-webresource-client-0.10.9 requests-cache-1.2.1 url-normalize-2.2.1\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.3\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "!pip install chembl-webresource-client\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kÃ¼tÃ¼phaneler: biopython, chembl_webresource_client, rdkit\n",
        "from Bio import Entrez, SeqIO\n",
        "import time\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MwFi22kv8_Pt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1ï¸âƒ£ **Biyoinformatik Veri Toplama: ABL1 Proteini**\n",
        "\n",
        "<div style=\"background-color: #e8f6f3; padding: 15px; border-left: 5px solid #1abc9c; border-radius: 5px;\">\n",
        "<strong>Hedef:</strong> NCBI veritabanÄ±nÄ± kullanarak LÃ¶semi ile iliÅŸkili <code>NP_005148</code> kodlu ABL1 proteininin FASTA sekansÄ±nÄ± Ã§ekmek.\n",
        "</div>"
      ],
      "metadata": {
        "id": "fk5ER3sS9X_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_genes = ['ABL1', 'FLT3', 'BTK', 'KIT', 'BCL2', 'JAK2']\n",
        "\n",
        "# NCBI iÃ§in Gen ID'leri (FASTA Ã§ekmek istersen diye)\n",
        "# Bu ID'ler insan (Homo sapiens) protein referanslarÄ±dÄ±r.\n",
        "ncbi_protein_ids = {\n",
        "    'ABL1': 'NP_005148',\n",
        "    'FLT3': 'NP_004110',\n",
        "    'BTK':  'NP_000052',\n",
        "    'KIT':  'NP_000213',\n",
        "    'BCL2': 'NP_000624',\n",
        "    'JAK2': 'NP_004963'\n",
        "}\n",
        "\n",
        "Entrez.email = \"240542017@firat.edu.com\" # BurayÄ± kendi mailinle doldur\n",
        "\n",
        "# --- KISIM 1: Ä°LAÃ‡ VERÄ°LERÄ°NÄ° (SMILES) TOPLAMA ---\n",
        "all_smiles_data = []\n",
        "\n",
        "print(\"Ä°laÃ§ verileri ChEMBL Ã¼zerinden Ã§ekiliyor...\")\n",
        "\n",
        "for gene in target_genes:\n",
        "    print(f\"  -> {gene} taranÄ±yor...\")\n",
        "\n",
        "    # 1. Hedef ID'sini bul\n",
        "    target = new_client.target\n",
        "    target_query = target.filter(target_synonym__icontains=gene).filter(target_organism__icontains='Homo sapiens')\n",
        "\n",
        "    if not target_query:\n",
        "        print(f\"     UYARI: {gene} ChEMBL'de bulunamadÄ±.\")\n",
        "        continue\n",
        "\n",
        "    target_id = target_query[0]['target_chembl_id']\n",
        "\n",
        "    # 2. Bu hedefe ait aktiviteleri (IC50) bul\n",
        "    activity = new_client.activity\n",
        "    res = activity.filter(target_chembl_id=target_id).filter(standard_type=\"IC50\")\n",
        "\n",
        "    # 3. SMILES kodlarÄ±nÄ± listeye ekle\n",
        "    # Her kaydÄ±n yanÄ±na hangi gen iÃ§in Ã§ekildiÄŸini de ekliyoruz (meta-data)\n",
        "    count = 0\n",
        "    for item in res:\n",
        "        if item['canonical_smiles']:\n",
        "            all_smiles_data.append({\n",
        "                'Gene': gene,\n",
        "                'SMILES': item['canonical_smiles'],\n",
        "                'IC50': item['value'] # Ä°lacÄ±n gÃ¼cÃ¼ (opsiyonel, filtrelemede kullanÄ±labilir)\n",
        "            })\n",
        "            count += 1\n",
        "    print(f\"     {count} molekÃ¼l eklendi.\")\n",
        "\n",
        "# DataFrame oluÅŸtur\n",
        "df_all = pd.DataFrame(all_smiles_data)\n",
        "print(f\"\\nTOPLAM HAM VERÄ°: {len(df_all)} satÄ±r.\")\n",
        "\n",
        "# --- KISIM 2: FASTA (PROTEÄ°N DÄ°ZÄ°LERÄ°) TOPLAMA ---\n",
        "print(\"\\nProtein dizileri NCBI Ã¼zerinden Ã§ekiliyor...\")\n",
        "\n",
        "with open(\"leukemia_targets_combined.fasta\", \"w\") as fasta_file:\n",
        "    for gene, ref_id in ncbi_protein_ids.items():\n",
        "        try:\n",
        "            handle = Entrez.efetch(db=\"protein\", id=ref_id, rettype=\"fasta\", retmode=\"text\")\n",
        "            seq_data = handle.read()\n",
        "            fasta_file.write(seq_data) # Dosyaya ekle (append mantÄ±ÄŸÄ±)\n",
        "            print(f\"  -> {gene} ({ref_id}) eklendi.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> HATA: {gene} Ã§ekilemedi. {e}\")\n",
        "\n",
        "print(\"TÃ¼m proteinler 'leukemia_targets_combined.fasta' dosyasÄ±na kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlZFTQ289eC_",
        "outputId": "6703519a-7765-4132-f0ea-16d6a64ae394"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ä°laÃ§ verileri ChEMBL Ã¼zerinden Ã§ekiliyor...\n",
            "  -> ABL1 taranÄ±yor...\n",
            "     5287 molekÃ¼l eklendi.\n",
            "  -> FLT3 taranÄ±yor...\n",
            "     7094 molekÃ¼l eklendi.\n",
            "  -> BTK taranÄ±yor...\n",
            "     21606 molekÃ¼l eklendi.\n",
            "  -> KIT taranÄ±yor...\n",
            "     4363 molekÃ¼l eklendi.\n",
            "  -> BCL2 taranÄ±yor...\n",
            "     65 molekÃ¼l eklendi.\n",
            "  -> JAK2 taranÄ±yor...\n",
            "     19204 molekÃ¼l eklendi.\n",
            "\n",
            "TOPLAM HAM VERÄ°: 57619 satÄ±r.\n",
            "\n",
            "Protein dizileri NCBI Ã¼zerinden Ã§ekiliyor...\n",
            "  -> ABL1 (NP_005148) eklendi.\n",
            "  -> FLT3 (NP_004110) eklendi.\n",
            "  -> BTK (NP_000052) eklendi.\n",
            "  -> KIT (NP_000213) eklendi.\n",
            "  -> BCL2 (NP_000624) eklendi.\n",
            "  -> JAK2 (NP_004963) eklendi.\n",
            "TÃ¼m proteinler 'leukemia_targets_combined.fasta' dosyasÄ±na kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "\n",
        "def canonicalize_smiles(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "print(\"Veri temizleniyor ve kanonikleÅŸtiriliyor...\")\n",
        "\n",
        "# 1. Canonical SMILES oluÅŸtur\n",
        "df_all['Canonical_SMILES'] = df_all['SMILES'].apply(canonicalize_smiles)\n",
        "\n",
        "# 2. GeÃ§ersiz olanlarÄ± at\n",
        "df_all = df_all.dropna(subset=['Canonical_SMILES'])\n",
        "\n",
        "# 3. KOPYALARI SÄ°L (En Ã¶nemli adÄ±m)\n",
        "# AynÄ± Canonical_SMILES'a sahip satÄ±rlarÄ± sil, sadece birini tut.\n",
        "df_clean_all = df_all.drop_duplicates(subset=['Canonical_SMILES'])\n",
        "\n",
        "print(f\"Temizlendikten sonraki EÅSÄ°Z molekÃ¼l sayÄ±sÄ±: {len(df_clean_all)}\")\n",
        "\n",
        "# 4. Kaydet\n",
        "df_clean_all.to_csv('leukemia_multitarget_dataset.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQJi0GB7KIlH",
        "outputId": "68481daf-f7ad-4d35-8c53-84be86080f9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri temizleniyor ve kanonikleÅŸtiriliyor...\n",
            "Temizlendikten sonraki EÅSÄ°Z molekÃ¼l sayÄ±sÄ±: 32437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize_smiles(smiles):\n",
        "    pattern = r\"(\\[[^\\[\\]]{1,6}\\])\"\n",
        "    tokens = re.split(pattern, smiles)\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('['):\n",
        "            result.append(token)\n",
        "        else:\n",
        "            result.extend(list(token))\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "print(tokenize_smiles('CC(=O)OC1=CC=CC=C1C(=O)O'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcHnKN0sKzuL",
        "outputId": "1115bcf5-20f6-4cdc-bfc8-e78c45cd408f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['C', 'C', '(', '=', 'O', ')', 'O', 'C', '1', '=', 'C', 'C', '=', 'C', 'C', '=', 'C', '1', 'C', '(', '=', 'O', ')', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TÃ¼m veri setindeki SMILES'larÄ± tarayÄ±p benzersiz tokenleri bulalÄ±m\n",
        "all_smiles = df_clean_all['Canonical_SMILES'].tolist()\n",
        "unique_tokens = set()\n",
        "\n",
        "for smile in all_smiles:\n",
        "    tokens = tokenize_smiles(smile)\n",
        "    unique_tokens.update(tokens)\n",
        "\n",
        "# Ã–zel tokenleri ekleyelim\n",
        "# <PAD>: Dolgu (0), <SOS>: BaÅŸlangÄ±Ã§, <EOS>: BitiÅŸ\n",
        "vocab = sorted(list(unique_tokens))\n",
        "vocab = ['<PAD>', '<SOS>', '<EOS>'] + vocab\n",
        "\n",
        "# Token -> ID (stoi) ve ID -> Token (itos) sÃ¶zlÃ¼klerini kuralÄ±m\n",
        "stoi = { token:i for i, token in enumerate(vocab) }\n",
        "itos = { i:token for i, token in enumerate(vocab) }\n",
        "\n",
        "print(f\"Toplam Benzersiz Token SayÄ±sÄ± (Vocab Size): {len(vocab)}\")\n",
        "print(\"Ã–rnek Token ID'leri:\", list(stoi.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d47_Zt-sK1i1",
        "outputId": "e601c2e3-2d1d-434e-a0d1-666ec9625d80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam Benzersiz Token SayÄ±sÄ± (Vocab Size): 57\n",
            "Ã–rnek Token ID'leri: [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('#', 3), ('(', 4), (')', 5), ('-', 6), ('.', 7), ('/', 8), ('1', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Maksimum molekÃ¼l uzunluÄŸunu bulalÄ±m (Padding iÃ§in gerekli)\n",
        "max_len = max([len(tokenize_smiles(s)) for s in all_smiles]) + 2 # +2 SOS ve EOS iÃ§in\n",
        "print(f\"En uzun molekÃ¼l uzunluÄŸu: {max_len}\")\n",
        "\n",
        "def smile_to_vector(smile, vocab_stoi, max_len):\n",
        "    tokens = tokenize_smiles(smile)\n",
        "    # BaÅŸÄ±na <SOS>, sonuna <EOS> ekle\n",
        "    vector = [vocab_stoi['<SOS>']] + [vocab_stoi[t] for t in tokens] + [vocab_stoi['<EOS>']]\n",
        "\n",
        "    # Padding (Geri kalan kÄ±smÄ± <PAD> yani 0 ile doldur)\n",
        "    padding = [vocab_stoi['<PAD>']] * (max_len - len(vector))\n",
        "    return vector + padding\n",
        "\n",
        "# TÃ¼m veri setini vektÃ¶rleÅŸtirelim\n",
        "X_data = []\n",
        "for smile in all_smiles:\n",
        "    vec = smile_to_vector(smile, stoi, max_len)\n",
        "    X_data.append(vec)\n",
        "\n",
        "# Tensor'a Ã§evirelim (PyTorch formatÄ±)\n",
        "X_tensor = torch.tensor(X_data, dtype=torch.long)\n",
        "print(f\"Veri Seti Boyutu: {X_tensor.shape}\") # (2851, max_len) olmalÄ±"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtES9qf7K8IO",
        "outputId": "4508cac8-443f-4fbc-d30b-7b8280b16622"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En uzun molekÃ¼l uzunluÄŸu: 210\n",
            "Veri Seti Boyutu: torch.Size([32437, 210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DrugDataset(Dataset):\n",
        "    def __init__(self, data_tensor):\n",
        "        self.data = data_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Girdi (Input): MolekÃ¼lÃ¼n baÅŸÄ±ndan sonuna kadar (son karakter hariÃ§)\n",
        "        # Hedef (Target): MolekÃ¼lÃ¼n 2. karakterinden sonuna kadar (bir adÄ±m kaydÄ±rÄ±lmÄ±ÅŸ)\n",
        "        # Model 'A'yÄ± gÃ¶rÃ¼nce 'B'yi tahmin etmeye Ã§alÄ±ÅŸacak.\n",
        "\n",
        "        full_seq = self.data[idx]\n",
        "        x = full_seq[:-1] # Girdi\n",
        "        y = full_seq[1:]  # Hedef (Sonraki karakter)\n",
        "        return x, y\n",
        "\n",
        "# Dataset ve DataLoader oluÅŸturma\n",
        "batch_size = 32 # T4 GPU iÃ§in uygun bir boyut\n",
        "dataset = DrugDataset(X_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test edelim\n",
        "sample_x, sample_y = next(iter(dataloader))\n",
        "print(\"Girdi (X) Boyutu:\", sample_x.shape)\n",
        "print(\"Hedef (Y) Boyutu:\", sample_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKnV2KVFLBLw",
        "outputId": "8f81d9ac-d695-42a2-8806-4d42d24b231c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Girdi (X) Boyutu: torch.Size([32, 209])\n",
            "Hedef (Y) Boyutu: torch.Size([32, 209])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5ï¸âƒ£ **LSTM Model Mimarisi (Generative AI)**\n",
        "<div style=\"background-color: #fadbd8; padding: 15px; border-left: 5px solid #e74c3c; border-radius: 5px;\">\n",
        "Modelimiz, bir Ã¶nceki karaktere bakarak bir sonraki karakteri tahmin etmeye Ã§alÄ±ÅŸan bir <strong>RNN (LSTM)</strong> aÄŸÄ±dÄ±r.\n",
        "</div>\n",
        "\n",
        "* **Embedding Layer:** Karakterleri yoÄŸun (dense) vektÃ¶rlere Ã§evirir.\n",
        "* **LSTM Layer:** Dizi bilgisini ve uzun sÃ¼reli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenir.\n",
        "* **Linear Layer:** SonuÃ§larÄ± tekrar Vocabulary boyutuna indirger."
      ],
      "metadata": {
        "id": "oO49oV8VLFJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MoleculeGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(MoleculeGenerator, self).__init__()\n",
        "\n",
        "        # 1. Embedding: SayÄ±larÄ± vektÃ¶rlere Ã§evirir (Ã–rn: 5 -> [0.1, -0.5, ...])\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # 2. LSTM: Dizideki iliÅŸkiyi Ã¶ÄŸrenir\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # 3. Linear: Sonraki karakterin olasÄ±lÄ±ÄŸÄ±nÄ± hesaplar\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        embed = self.embedding(x)\n",
        "\n",
        "        # out shape: (batch_size, seq_len, hidden_size)\n",
        "        out, (ht, ct) = self.lstm(embed, hidden)\n",
        "\n",
        "        # output shape: (batch_size, seq_len, vocab_size)\n",
        "        output = self.fc(out)\n",
        "        return output, (ht, ct)\n",
        "\n",
        "# Model Parametreleri\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_SIZE = 128   # Her token ne kadar detaylÄ± temsil edilsin?\n",
        "HIDDEN_SIZE = 256  # Modelin hafÄ±za kapasitesi\n",
        "NUM_LAYERS = 2     # KaÃ§ katmanlÄ± LSTM?\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MoleculeGenerator(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
        "\n",
        "print(model)\n",
        "print(f\"Model {device} Ã¼zerinde Ã§alÄ±ÅŸacak.\")"
      ],
      "metadata": {
        "id": "GFXz0ie3LELS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ‹ï¸â€â™‚ï¸ Model EÄŸitimi\n",
        "Model, ABL1 inhibitÃ¶rlerinin SMILES yapÄ±larÄ±nÄ± Ã¶ÄŸrenmek Ã¼zere eÄŸitiliyor. Her 2 epoch'ta bir Ã¶rnek Ã¼retim yaparak geliÅŸimi gÃ¶zlemleyeceÄŸiz."
      ],
      "metadata": {
        "id": "-pF7hZa5LKIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Hiperparametreler ve Ayarlar\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 1000\n",
        "\n",
        "# EÄŸer daha Ã¶nce tanÄ±mlanmadÄ±ysa tekrar tanÄ±mlayalÄ±m\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train(model, dataloader, epochs):\n",
        "    model.train() # BaÅŸlangÄ±Ã§ta eÄŸitim modu\n",
        "    loss_history = []\n",
        "    print(f\"EÄŸitim BaÅŸlÄ±yor... ({device})\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(x)\n",
        "\n",
        "            loss = criterion(output.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        loss_history.append(avg_loss)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        if (epoch+1) % 2 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | SÃ¼re: {elapsed:.1f}sn\")\n",
        "\n",
        "            # Her 2 epochta bir Ã¶rnek Ã¼retip geliÅŸimini gÃ¶relim (SÄ±klÄ±ÄŸÄ± artÄ±rdÄ±m)\n",
        "            if (epoch+1) % 2 == 0:\n",
        "                print(\"   -> Ã–rnek Ã¼retiliyor...\")\n",
        "\n",
        "                # 1. Ã–rnek Ãœret (Bu iÅŸlem model.eval() yapar)\n",
        "                test_smiles = generate_molecule(model, start_str=\"C\", temperature=0.8)\n",
        "                print(f\"   -> SonuÃ§: {test_smiles}\")\n",
        "\n",
        "                # 2. KRÄ°TÄ°K DÃœZELTME: Modeli tekrar eÄŸitim moduna al!\n",
        "                model.train()\n",
        "                # ^^^ BU SATIR EKSÄ°KTÄ° ^^^\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "# EÄŸitimi tekrar baÅŸlat (Modelin kaldÄ±ÄŸÄ± yerden devam eder ama temiz olsun dersen modeli tekrar baÅŸtan tanÄ±mlayabilirsin)\n",
        "loss_history = train(model, dataloader, EPOCHS)\n",
        "\n",
        "# Modeli Kaydet\n",
        "torch.save(model.state_dict(), \"oncomind_model.pth\")\n",
        "print(\"Model baÅŸarÄ±yla kaydedildi!\")"
      ],
      "metadata": {
        "id": "kcfoUY26LIB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6ï¸âƒ£ **Validasyon ve Ä°laÃ§ Uygunluk Testi**\n",
        "Ãœretilen molekÃ¼llerin geÃ§erli kimyasal yapÄ±lar olup olmadÄ±ÄŸÄ±nÄ± ve ilaÃ§ olmaya adaylÄ±k skorlarÄ±nÄ± (QED) kontrol ediyoruz."
      ],
      "metadata": {
        "id": "bwVPHiykLQek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  rdkit  import  Chem\n",
        "from  rdkit.Chem  import  Draw\n",
        "\n",
        "mol  =  Chem.MolFromSmiles(\"CC(C)(C)C(=O)N1Cc2c(NC(=O)c3cc(F)cc(F)c3)n[nH]c2C1(C)C\")\n",
        "Draw.MolToImage(mol)"
      ],
      "metadata": {
        "id": "Z9_2TacHLN1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_molecule(model, start_str=\"C\", max_length=100, temperature=1.0):\n",
        "    model.eval() # DeÄŸerlendirme modu\n",
        "\n",
        "    # BaÅŸlangÄ±Ã§ karakterini vektÃ¶re Ã§evir\n",
        "    input_seq = [stoi['<SOS>']] + [stoi[c] for c in start_str]\n",
        "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_str = start_str\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "\n",
        "            # Son karakterin tahminini al\n",
        "            last_token_logits = output[0, -1, :] / temperature\n",
        "            probs = torch.nn.functional.softmax(last_token_logits, dim=0)\n",
        "\n",
        "            # OlasÄ±lÄ±klara gÃ¶re bir sonraki karakteri seÃ§\n",
        "            next_token_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            if next_token_idx == stoi['<EOS>']:\n",
        "                break\n",
        "\n",
        "            next_char = itos[next_token_idx]\n",
        "            generated_str += next_char\n",
        "\n",
        "            # Yeni girdiyi gÃ¼ncelle\n",
        "            input_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "generated_molecule = generate_molecule(model, start_str=\"C\", temperature=0.8)\n",
        "print(\"Ãœretilen Ã–rnek:\", generated_molecule)"
      ],
      "metadata": {
        "id": "P1QExMZYLTxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelin Ã¼rettiÄŸi Ã§Ä±ktÄ±yÄ± analiz etme ve eleme kÄ±smÄ± (Organ yetmezliÄŸi kÄ±sÄ±tÄ±)\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, QED\n",
        "\n",
        "def check_drug_quality(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None: return False\n",
        "\n",
        "    # 1. QED (Quantitative Estimation of Drug-likeness)\n",
        "    # Bu skor ilacÄ±n genel olarak vÃ¼cutta emilip emilemeyeceÄŸini tahmin eder.\n",
        "    qed_score = QED.qed(mol)\n",
        "\n",
        "    # 2. MolekÃ¼ler AÄŸÄ±rlÄ±k (Ã‡ok aÄŸÄ±rsa bÃ¶breklerden atÄ±lamaz)\n",
        "    mw = Descriptors.MolWt(mol)\n",
        "\n",
        "    # Filtre: Ä°laÃ§ benzeri olsun VE Ã§ok aÄŸÄ±r olmasÄ±n (Basit bir toksisite Ã¶nlemi)\n",
        "    if qed_score > 0.5 and mw < 500:\n",
        "        return True, f\"MolekÃ¼l uygun. QED: {qed_score:.2f}, MW: {mw:.1f}\"\n",
        "    else:\n",
        "        return False, \"Uygunsuz.\"\n",
        "\n",
        "# Ã–rnek KullanÄ±m\n",
        "generated_molecule = \"CC(C)(C)C(=O)N1Cc2c(NC(=O)c3cc(F)cc(F)c3)n[nH]c2C1(C)C\" # (Imatinib benzeri bir yapÄ±)\n",
        "is_good, reason = check_drug_quality(generated_molecule)\n",
        "print(reason)"
      ],
      "metadata": {
        "id": "WF1Db17dLVWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}